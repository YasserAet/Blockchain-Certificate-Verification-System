# ML Quick Start Guide - 3 Simple Use Cases

## Overview
You DON'T need to train models from scratch! We'll use **pre-trained models** and adapt them to our data.

## üéØ Use Case 1: Document Classification (Certificate vs Non-Certificate)

### What it does
Automatically identifies if an uploaded image is a certificate or not.

### Dataset
- **Source**: https://www.kaggle.com/datasets/techsash/certificate-dataset
- **Size**: ~1000 certificate images
- **Additional**: Add 500 non-certificate images (receipts, invoices, photos)

### Step-by-Step Implementation

#### Step 1: Download Dataset (5 minutes)
```bash
# Install Kaggle CLI
pip install kaggle

# Set up Kaggle API key (get from https://www.kaggle.com/settings)
# Place kaggle.json in ~/.kaggle/

# Download dataset
kaggle datasets download -d techsash/certificate-dataset
unzip certificate-dataset.zip -d datasets/certificates/
```

#### Step 2: Organize Data (2 minutes)
```
datasets/
‚îî‚îÄ‚îÄ document_classification/
    ‚îú‚îÄ‚îÄ train/
    ‚îÇ   ‚îú‚îÄ‚îÄ certificate/        # 800 images
    ‚îÇ   ‚îî‚îÄ‚îÄ non_certificate/    # 400 images
    ‚îî‚îÄ‚îÄ validation/
        ‚îú‚îÄ‚îÄ certificate/        # 200 images
        ‚îî‚îÄ‚îÄ non_certificate/    # 100 images
```

#### Step 3: Use Pre-trained Model (NO TRAINING NEEDED!)
```python
# ml-service/src/models/document_classifier.py

from transformers import pipeline
from PIL import Image

# Use pre-trained image classification model
classifier = pipeline("image-classification", model="microsoft/resnet-50")

def classify_document(image_path: str) -> dict:
    """
    Classify if image is a certificate or not.
    NO TRAINING REQUIRED - uses pre-trained model!
    """
    image = Image.open(image_path)
    results = classifier(image)
    
    # Simple rule: if top prediction contains 'document' or 'certificate'
    is_certificate = any(
        keyword in results[0]['label'].lower() 
        for keyword in ['document', 'certificate', 'diploma', 'paper']
    )
    
    return {
        'is_certificate': is_certificate,
        'confidence': results[0]['score'],
        'predictions': results
    }

# Usage
result = classify_document('uploads/certificate.jpg')
print(f"Is certificate: {result['is_certificate']}")
```

#### Step 4 (OPTIONAL): Fine-tune for Better Accuracy
Only do this if pre-trained model accuracy is < 80%

```python
# Fine-tuning script (OPTIONAL)
from transformers import AutoImageProcessor, AutoModelForImageClassification, Trainer

# Load pre-trained model
model = AutoModelForImageClassification.from_pretrained(
    "microsoft/resnet-50",
    num_labels=2,  # certificate vs non-certificate
    ignore_mismatched_sizes=True
)

# Fine-tune on your data (takes ~30 minutes on GPU, 2 hours on CPU)
# See full script in ml-service/scripts/train_classifier.py
```

**Time Required**:
- ‚úÖ Using pre-trained: 10 minutes
- üîÑ Fine-tuning (optional): 2-3 hours

---

## üéØ Use Case 2: Skill Extraction from Coursera Certificates

### What it does
Extracts skills/courses from certificate text (e.g., "Python Programming", "Machine Learning")

### Dataset
- **Source**: https://www.kaggle.com/datasets/PromptCloudHQ/udemy-courses-dataset-50k-courses
- **Coursera Alternative**: https://www.kaggle.com/datasets/khusheekapoor/coursera-courses-dataset-2021
- **Size**: 50K+ courses with skills listed

#### Step 1: Download Dataset (3 minutes)
```bash
kaggle datasets download -d khusheekapoor/coursera-courses-dataset-2021
unzip coursera-courses-dataset-2021.zip -d datasets/skills/
```

#### Step 2: Build Skill Database (5 minutes)
```python
# ml-service/src/models/skill_extractor.py

import pandas as pd
from sentence_transformers import SentenceTransformer, util

# Load skill database
df = pd.read_csv('datasets/skills/coursera_data.csv')
all_skills = df['course_skills'].dropna().str.split(',').explode().unique()

# Use pre-trained embedding model (NO TRAINING!)
model = SentenceTransformer('all-MiniLM-L6-v2')
skill_embeddings = model.encode(all_skills)

def extract_skills_from_text(certificate_text: str) -> list:
    """
    Extract skills from OCR text using semantic similarity.
    NO TRAINING REQUIRED!
    """
    # Encode the certificate text
    text_embedding = model.encode(certificate_text)
    
    # Find similar skills
    similarities = util.cos_sim(text_embedding, skill_embeddings)[0]
    
    # Get top 5 matching skills
    top_matches = similarities.argsort(descending=True)[:5]
    
    extracted_skills = [
        {
            'skill': all_skills[idx],
            'confidence': float(similarities[idx])
        }
        for idx in top_matches
        if similarities[idx] > 0.5  # Only keep if similarity > 50%
    ]
    
    return extracted_skills

# Usage
text = "Certificate of Completion: Python for Data Science and Machine Learning"
skills = extract_skills_from_text(text)
print(skills)
# Output: [
#   {'skill': 'Python Programming', 'confidence': 0.89},
#   {'skill': 'Machine Learning', 'confidence': 0.85},
#   {'skill': 'Data Science', 'confidence': 0.82}
# ]
```

**Time Required**: 15 minutes (no training needed!)

---

## üéØ Use Case 3: OCR (Extract Text from Certificates)

### What it does
Converts certificate images to text (name, date, course, institution)

### Dataset (for testing only)
- **Source**: https://rrc.cvc.uab.es/ (ICDAR datasets)
- **Note**: We use pre-trained OCR models, dataset is only for accuracy testing

#### Step 1: Install Pre-trained OCR (NO TRAINING!)
```bash
# Option 1: Tesseract (fastest, good for printed text)
pip install pytesseract
# Download: https://github.com/UB-Mannheim/tesseract/wiki

# Option 2: EasyOCR (best for mixed languages)
pip install easyocr

# Option 3: TrOCR (best accuracy, slower)
pip install transformers torch
```

#### Step 2: Use Pre-trained OCR Models
```python
# ml-service/src/models/ocr_extractor.py

import easyocr
from PIL import Image

# Initialize pre-trained reader (downloads automatically)
reader = easyocr.Reader(['en', 'fr', 'ar'])  # English, French, Arabic

def extract_text_from_certificate(image_path: str) -> dict:
    """
    Extract text from certificate using pre-trained OCR.
    NO TRAINING REQUIRED!
    """
    # Run OCR
    results = reader.readtext(image_path)
    
    # Extract text and bounding boxes
    extracted_data = {
        'full_text': ' '.join([text for (bbox, text, conf) in results]),
        'fields': []
    }
    
    for (bbox, text, confidence) in results:
        extracted_data['fields'].append({
            'text': text,
            'confidence': confidence,
            'position': bbox
        })
    
    return extracted_data

# Usage
result = extract_text_from_certificate('uploads/certificate.jpg')
print(result['full_text'])
```

#### Step 3: Structured Data Extraction (Optional Enhancement)
```python
import re
from datetime import datetime

def parse_certificate_fields(ocr_text: str) -> dict:
    """
    Parse OCR text into structured fields.
    Uses regex patterns - NO ML TRAINING NEEDED!
    """
    data = {
        'name': None,
        'course': None,
        'date': None,
        'institution': None
    }
    
    # Extract date (various formats)
    date_patterns = [
        r'\d{1,2}[/-]\d{1,2}[/-]\d{2,4}',  # 12/31/2024
        r'\d{4}-\d{2}-\d{2}',                # 2024-12-31
        r'[A-Z][a-z]+ \d{1,2}, \d{4}'       # January 31, 2024
    ]
    for pattern in date_patterns:
        match = re.search(pattern, ocr_text)
        if match:
            data['date'] = match.group()
            break
    
    # Extract name (after "awarded to" or "presented to")
    name_patterns = [
        r'(?:awarded to|presented to|this certifies that)\s+([A-Z][a-z]+(?: [A-Z][a-z]+)+)',
        r'Name:?\s+([A-Z][a-z]+(?: [A-Z][a-z]+)+)'
    ]
    for pattern in name_patterns:
        match = re.search(pattern, ocr_text, re.IGNORECASE)
        if match:
            data['name'] = match.group(1)
            break
    
    # Extract course (usually in title case or all caps)
    lines = ocr_text.split('\n')
    for line in lines:
        if 5 < len(line) < 100 and line.isupper():
            data['course'] = line
            break
    
    return data

# Usage
text = extract_text_from_certificate('certificate.jpg')['full_text']
fields = parse_certificate_fields(text)
print(fields)
# Output: {
#   'name': 'John Doe',
#   'course': 'PYTHON PROGRAMMING',
#   'date': 'January 15, 2024',
#   'institution': 'Coursera'
# }
```

**Time Required**: 10 minutes (no training needed!)

---

## üöÄ Quick Integration into Your System

### Create ML Service API Endpoints

```python
# ml-service/app.py

from fastapi import FastAPI, File, UploadFile
from document_classifier import classify_document
from skill_extractor import extract_skills_from_text
from ocr_extractor import extract_text_from_certificate, parse_certificate_fields
import shutil

app = FastAPI()

@app.post("/api/ml/classify-document")
async def classify_uploaded_document(file: UploadFile = File(...)):
    """Use Case 1: Check if uploaded file is a certificate"""
    # Save uploaded file
    temp_path = f"temp/{file.filename}"
    with open(temp_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    
    # Classify
    result = classify_document(temp_path)
    return result

@app.post("/api/ml/extract-certificate-data")
async def extract_certificate_info(file: UploadFile = File(...)):
    """Use Case 3: Extract text from certificate"""
    temp_path = f"temp/{file.filename}"
    with open(temp_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    
    # OCR
    ocr_result = extract_text_from_certificate(temp_path)
    parsed_data = parse_certificate_fields(ocr_result['full_text'])
    
    return {
        'ocr_text': ocr_result['full_text'],
        'extracted_fields': parsed_data
    }

@app.post("/api/ml/extract-skills")
async def extract_skills(certificate_text: str):
    """Use Case 2: Extract skills from certificate text"""
    skills = extract_skills_from_text(certificate_text)
    return {'skills': skills}

# Start server
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

---

## üìã Implementation Roadmap (1 Week)

### Day 1: Setup (2 hours)
- [ ] Install Python packages: `pip install transformers sentence-transformers easyocr pandas`
- [ ] Download Coursera skills dataset
- [ ] Download certificate sample images (50-100 samples)
- [ ] Test EasyOCR on 5 sample certificates

### Day 2: Use Case 3 - OCR (3 hours)
- [ ] Implement `ocr_extractor.py`
- [ ] Test on 20 different certificates
- [ ] Measure accuracy (should be >90% for printed text)
- [ ] Add API endpoint `/api/ml/extract-certificate-data`

### Day 3: Use Case 2 - Skill Extraction (2 hours)
- [ ] Load Coursera dataset into DataFrame
- [ ] Implement `skill_extractor.py`
- [ ] Test with 10 certificate texts
- [ ] Add API endpoint `/api/ml/extract-skills`

### Day 4: Use Case 1 - Document Classification (3 hours)
- [ ] Download certificate dataset
- [ ] Implement `document_classifier.py` with pre-trained ResNet
- [ ] Test accuracy on 50 samples
- [ ] Add API endpoint `/api/ml/classify-document`

### Day 5: Integration Testing (2 hours)
- [ ] Connect ML service to backend
- [ ] Test full flow: Upload ‚Üí Classify ‚Üí OCR ‚Üí Extract Skills
- [ ] Fix bugs and edge cases

### Day 6-7: Optional Fine-tuning
- [ ] If accuracy < 80%, fine-tune models
- [ ] Otherwise, move to Phase 3 features

---

## üéì Key Principles

### ‚úÖ DO:
1. **Use pre-trained models first** - Don't train from scratch!
2. **Test on small samples** - Validate before scaling
3. **Start simple** - OCR + Regex > Complex AI initially
4. **Measure accuracy** - Know when models fail

### ‚ùå DON'T:
1. **Don't train models from scratch** - Waste of time/resources
2. **Don't download huge datasets** - Start with 100-500 samples
3. **Don't obsess over perfection** - 80% accuracy is often enough
4. **Don't skip testing** - Always validate on real certificates

---

## üìä Expected Accuracy (Pre-trained Models)

| Task | Pre-trained Accuracy | After Fine-tuning |
|------|---------------------|-------------------|
| Document Classification | 70-80% | 90-95% |
| OCR (Printed Text) | 90-95% | 95-98% |
| OCR (Handwritten) | 60-70% | 75-85% |
| Skill Extraction | 75-85% | 85-90% |

---

## üõ†Ô∏è Next Steps

### Immediate (Today):
```bash
# 1. Install core libraries
pip install transformers sentence-transformers easyocr pandas fastapi uvicorn

# 2. Test OCR on one certificate
python -c "
import easyocr
reader = easyocr.Reader(['en'])
result = reader.readtext('path/to/certificate.jpg')
print(result)
"

# 3. Download Coursera dataset
kaggle datasets download -d khusheekapoor/coursera-courses-dataset-2021
```

### This Week:
- Implement all 3 use cases
- Create ML API endpoints
- Test with real certificate samples

### No Training Needed!
All models are **pre-trained** and work out-of-the-box. Fine-tuning is optional and only needed if accuracy is too low.
